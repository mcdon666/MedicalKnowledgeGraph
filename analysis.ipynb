{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "\n",
    "# 连接数据库\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# 查询所有节点和边\n",
    "query = \"\"\"\n",
    "MATCH (a)-[r]->(b)\n",
    "RETURN id(a) as source, id(b) as target, type(r) as relation, properties(r) as edge_attr\n",
    "\"\"\"\n",
    "edges = graph.run(query).data()\n",
    "\n",
    "# 构建 NetworkX 图\n",
    "G = nx.DiGraph()\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[\"source\"], edge[\"target\"], **(edge[\"edge_attr\"] or {}))\n",
    "\n",
    "# 添加节点属性\n",
    "node_query = \"MATCH (n) RETURN id(n) as id, labels(n) as labels, properties(n) as attr\"\n",
    "nodes = graph.run(node_query).data()\n",
    "\n",
    "for node in nodes:\n",
    "    G.add_node(node[\"id\"], **node[\"attr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 图结构统计信息\n",
      "👉 节点总数：44112\n",
      "👉 边总数：276586\n",
      "👉 平均入度：6.27\n",
      "👉 平均出度：6.27\n",
      "👉 各类型节点数量：\n",
      "   - Disease: 8808 个\n",
      "   - Drug: 3828 个\n",
      "   - Food: 4870 个\n",
      "   - Check: 3353 个\n",
      "   - Department: 54 个\n",
      "   - Producer: 17201 个\n",
      "   - Symptom: 5998 个\n",
      "👉 各类型关系数量：\n",
      "   - recommand_eat: 40236 条\n",
      "   - no_eat: 22247 条\n",
      "   - do_eat: 22238 条\n",
      "   - belongs_to: 8844 条\n",
      "   - common_drug: 14649 条\n",
      "   - drugs_of: 17315 条\n",
      "   - recommand_drug: 59467 条\n",
      "   - need_check: 39423 条\n",
      "   - has_symptom: 54717 条\n",
      "   - acompany_with: 12029 条\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 图结构基本统计信息输出\n",
    "# -----------------------------\n",
    "\n",
    "print(\"✅ 图结构统计信息\")\n",
    "print(f\"👉 节点总数：{G.number_of_nodes()}\")\n",
    "print(f\"👉 边总数：{G.number_of_edges()}\")\n",
    "\n",
    "# 节点入度、出度\n",
    "in_degrees = dict(G.in_degree())\n",
    "out_degrees = dict(G.out_degree())\n",
    "\n",
    "avg_in_degree = sum(in_degrees.values()) / len(in_degrees)\n",
    "avg_out_degree = sum(out_degrees.values()) / len(out_degrees)\n",
    "\n",
    "print(f\"👉 平均入度：{avg_in_degree:.2f}\")\n",
    "print(f\"👉 平均出度：{avg_out_degree:.2f}\")\n",
    "\n",
    "# 节点标签统计（如果有标签）\n",
    "label_count = {}\n",
    "for node in nodes:\n",
    "    for label in node[\"labels\"]:\n",
    "        label_count[label] = label_count.get(label, 0) + 1\n",
    "print(\"👉 各类型节点数量：\")\n",
    "for label, count in label_count.items():\n",
    "    print(f\"   - {label}: {count} 个\")\n",
    "\n",
    "# 关系类型统计\n",
    "relation_count = {}\n",
    "for edge in edges:\n",
    "    rel = edge['relation']\n",
    "    relation_count[rel] = relation_count.get(rel, 0) + 1\n",
    "print(\"👉 各类型关系数量：\")\n",
    "for rel, count in relation_count.items():\n",
    "    print(f\"   - {rel}: {count} 条\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👉 部门列表（共 54 个）：不孕不育、中医科、中医综合、五官科、产科、传染科、儿科、儿科综合、其他科室、其他综合、内分泌科、内科、减肥、口腔科、呼吸内科、外科、妇产科、妇科、小儿内科、小儿外科、康复科、心内科、心理科、心胸外科、急诊科、性病科、感染科、整形美容科、普外科、泌尿内科、泌尿外科、消化内科、烧伤科、生殖健康、男科、皮肤性病科、皮肤科、眼科、神经内科、神经外科、精神科、耳鼻喉科、肛肠科、肝病、肝胆外科、肾内科、肿瘤内科、肿瘤外科、肿瘤科、营养科、血液科、遗传病科、风湿免疫科、骨外科\n"
     ]
    }
   ],
   "source": [
    "# 收集所有 Department 类型的部门名\n",
    "department_names = []\n",
    "for node in nodes:\n",
    "    if \"Department\" in node[\"labels\"]:\n",
    "        dept_name = node[\"attr\"].get(\"name\") or node[\"attr\"].get(\"department_name\")\n",
    "        if dept_name:\n",
    "            department_names.append(dept_name)\n",
    "\n",
    "# 按字典序排序部门名\n",
    "department_names.sort()\n",
    "\n",
    "# 拼接成一个用顿号分隔的字符串\n",
    "departments_str = \"、\".join(department_names)\n",
    "\n",
    "print(f\"👉 部门列表（共 {len(department_names)} 个）：{departments_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据统计\n",
      "症状 (Symptom)节点数：5998\n",
      "疾病 (Disease)节点数：8765\n",
      "科室 (Department)节点数：48\n",
      "症状-疾病边数：54717\n",
      "疾病-科室边数：54717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 全流程：症状 → 疾病 → 科室预测模型构建\n",
    "# 使用 PyTorch Geometric + Neo4j + BERT 特征\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from py2neo import Graph\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# ========== 1. 连接 Neo4j 并提取数据 ==========\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"12345678\"))\n",
    "\n",
    "# 提取症状-疾病-科室三跳路径\n",
    "sym_dis_dept_query = \"\"\"\n",
    "MATCH (s:Symptom)<-[:has_symptom]-(d:Disease)-[:belongs_to]->(dept:Department)\n",
    "RETURN id(s) as sid, s.name as sname, id(d) as did, d.name as dname, id(dept) as dept_id, dept.name as dept_name\n",
    "\"\"\"\n",
    "results = graph.run(sym_dis_dept_query).data()\n",
    "\n",
    "# 构建实体字典与映射\n",
    "symptoms, diseases, departments = {}, {}, {}\n",
    "edges_sym_dis, edges_dis_dept = [], []\n",
    "\n",
    "for row in results:\n",
    "    sid, sname = row[\"sid\"], row[\"sname\"]\n",
    "    did, dname = row[\"did\"], row[\"dname\"]\n",
    "    dept_id, dept_name = row[\"dept_id\"], row[\"dept_name\"]\n",
    "    symptoms[sid] = sname\n",
    "    diseases[did] = dname\n",
    "    departments[dept_id] = dept_name\n",
    "    edges_sym_dis.append((sid, did))\n",
    "    edges_dis_dept.append((did, dept_id))\n",
    "\n",
    "symptom_id_map = {nid: i for i, nid in enumerate(symptoms)}\n",
    "disease_id_map = {nid: i for i, nid in enumerate(diseases)}\n",
    "department_id_map = {nid: i for i, nid in enumerate(departments)}\n",
    "\n",
    "# ========= 统计信息 =========\n",
    "print(\"数据统计\")\n",
    "print(f\"症状 (Symptom)节点数：{len(symptoms)}\")\n",
    "print(f\"疾病 (Disease)节点数：{len(diseases)}\")\n",
    "print(f\"科室 (Department)节点数：{len(departments)}\")\n",
    "print(f\"症状-疾病边数：{len(edges_sym_dis)}\")\n",
    "print(f\"疾病-科室边数：{len(edges_dis_dept)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 department 索引 → 名称 映射\n",
    "department_idx2name = {\n",
    "    idx: departments[nid]  # nid 是 Neo4j 中的节点 ID，departments[nid] 是名称\n",
    "    for nid, idx in department_id_map.items()\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open(\"department_idx2name.pkl\", \"wb\") as f:\n",
    "    pickle.dump(department_idx2name, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. 构建 HeteroData 异构图 ==========\n",
    "data = HeteroData()\n",
    "data['symptom'].num_nodes = len(symptoms)\n",
    "data['disease'].num_nodes = len(diseases)\n",
    "data['department'].num_nodes = len(departments)\n",
    "\n",
    "# 边：symptom <-> disease\n",
    "edge_index_sd = torch.tensor([[symptom_id_map[s], disease_id_map[d]] for s, d in edges_sym_dis], dtype=torch.long).t()\n",
    "data['symptom', 'has_symptom', 'disease'].edge_index = edge_index_sd\n",
    "\n",
    "# 边：disease <-> department\n",
    "edge_index_dd = torch.tensor([[disease_id_map[d], department_id_map[dept]] for d, dept in edges_dis_dept], dtype=torch.long).t()\n",
    "data['disease', 'belongs_to', 'department'].edge_index = edge_index_dd\n",
    "\n",
    "# 自动添加反向边\n",
    "data = ToUndirected()(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码 症状 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "症状:   0%|          | 0/5998 [00:00<?, ?it/s]g:\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "症状: 100%|██████████| 5998/5998 [00:38<00:00, 156.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 symptom_feat.pt 和 symptom_name2idx.pkl\n",
      "编码 疾病 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "疾病: 100%|██████████| 8765/8765 [00:56<00:00, 156.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 disease_feat.pt 和 disease_name2idx.pkl\n",
      "编码 科室 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "科室: 100%|██████████| 48/48 [00:00<00:00, 146.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 department_feat.pt 和 department_name2idx.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# ========== 3. 生成 BERT 特征 ==========\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "model.eval()  # 关闭 dropout 等训练态\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_bert_embedding(text: str) -> torch.Tensor:\n",
    "    \"\"\"对单个文本返回 [CLS] 向量 (1, 768)。\"\"\"\n",
    "    inputs  = tokenizer(text, return_tensors='pt', truncation=True,\n",
    "                        padding=True, max_length=10).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]    # shape: (1, 768)\n",
    "\n",
    "def build_or_load(name_dict, save_path, desc, name2idx_path):\n",
    "    if os.path.exists(save_path) and os.path.exists(name2idx_path):\n",
    "        print(f\"载入缓存特征：{save_path}\")\n",
    "        feats = torch.load(save_path)\n",
    "        with open(name2idx_path, \"rb\") as f:\n",
    "            name2idx = pickle.load(f)\n",
    "        return feats, name2idx\n",
    "\n",
    "    print(f\"编码 {desc} ...\")\n",
    "    feats = []\n",
    "    name2idx = {}\n",
    "    for i, (nid, name) in enumerate(tqdm(name_dict.items(), total=len(name_dict), desc=desc)):\n",
    "        vec = get_bert_embedding(name).cpu()\n",
    "        feats.append(vec.squeeze(0))\n",
    "        name2idx[name] = i\n",
    "\n",
    "    feats = torch.stack(feats)\n",
    "    torch.save(feats, save_path)\n",
    "    with open(name2idx_path, \"wb\") as f:\n",
    "        pickle.dump(name2idx, f)\n",
    "    print(f\"已保存到 {save_path} 和 {name2idx_path}\")\n",
    "    return feats, name2idx\n",
    "\n",
    "symptom_feat, symptom_name2idx = build_or_load(symptoms, \"symptom_feat.pt\", \"症状\", \"symptom_name2idx.pkl\")\n",
    "disease_feat, disease_name2idx = build_or_load(diseases, \"disease_feat.pt\", \"疾病\", \"disease_name2idx.pkl\")\n",
    "department_feat, department_name2idx = build_or_load(departments, \"department_feat.pt\", \"科室\", \"department_name2idx.pkl\")\n",
    "\n",
    "\n",
    "data['symptom'].x = symptom_feat\n",
    "data['disease'].x = disease_feat\n",
    "data['department'].x = department_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共构建有效标签样本数：5998\n"
     ]
    }
   ],
   "source": [
    "# 构造 symptom → department 的标签列表\n",
    "symptom_label = torch.full((len(symptoms),), -1, dtype=torch.long)  # -1 表示无标签（跳过）\n",
    "\n",
    "# 反向构建：通过疾病连接的科室\n",
    "for (s_id, d_id) in edges_sym_dis:\n",
    "    dept_ids = [dept for (did, dept) in edges_dis_dept if did == d_id]\n",
    "    if len(dept_ids) == 0:\n",
    "        continue\n",
    "    s_idx = symptom_id_map[s_id]\n",
    "    # 注意：存在多个疾病连接到不同科室的情况，仅取第一个（也可以改为多标签 one-hot）\n",
    "    dept_idx = department_id_map[dept_ids[0]]\n",
    "    symptom_label[s_idx] = dept_idx\n",
    "\n",
    "# 去除无标签的样本\n",
    "mask = symptom_label != -1\n",
    "labels = symptom_label[mask]\n",
    "print(f\"共构建有效标签样本数：{labels.size(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(symptom_label, \"symptom_label.pt\")\n",
    "torch.save(mask, \"mask.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\3342686644.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  symptom_label = torch.load(\"symptom_label.pt\")\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\3342686644.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mask = torch.load(\"mask.pt\")\n"
     ]
    }
   ],
   "source": [
    "symptom_label = torch.load(\"symptom_label.pt\")\n",
    "mask = torch.load(\"mask.pt\")\n",
    "labels = symptom_label[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本: 4798，测试集样本: 1200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 有效症状索引（mask=True 处）\n",
    "valid_sym_idx = torch.where(mask)[0]        # 5998 全部有效\n",
    "train_idx, test_idx = train_test_split(valid_sym_idx.numpy(),\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=42,\n",
    "                                       shuffle=True)\n",
    "\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_idx  = torch.tensor(test_idx,  dtype=torch.long)\n",
    "\n",
    "print(f\"训练集样本: {len(train_idx)}，测试集样本: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymptomToDeptGNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, out_dim=len(department_id_map)):\n",
    "        super().__init__()\n",
    "        self.conv = HeteroConv({\n",
    "            ('symptom', 'has_symptom', 'disease'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "            ('disease', 'rev_has_symptom', 'symptom'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "            ('disease', 'belongs_to', 'department'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "            ('department','rev_belongs_to','disease'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv(x_dict, edge_index_dict)    # 异构消息传播\n",
    "        x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "        return self.lin(x_dict['symptom'])             # 仅输出症状节点 logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练 ...\n",
      "Epoch 001 | Loss: 3.9007 | Test Acc: 0.0283\n",
      "Epoch 010 | Loss: 2.4937 | Test Acc: 0.3225\n",
      "Epoch 020 | Loss: 1.7720 | Test Acc: 0.4783\n",
      "Epoch 030 | Loss: 1.3902 | Test Acc: 0.5650\n",
      "Epoch 040 | Loss: 1.1775 | Test Acc: 0.5933\n",
      "Epoch 050 | Loss: 1.0255 | Test Acc: 0.6183\n",
      "Epoch 060 | Loss: 0.9062 | Test Acc: 0.6167\n",
      "Epoch 070 | Loss: 0.8177 | Test Acc: 0.6283\n",
      "Epoch 080 | Loss: 0.7301 | Test Acc: 0.6333\n",
      "Epoch 090 | Loss: 0.6662 | Test Acc: 0.6317\n",
      "Epoch 100 | Loss: 0.6098 | Test Acc: 0.6433\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "labels = labels.to(device)            # size = 5998\n",
    "train_idx = train_idx.to(device)\n",
    "test_idx  = test_idx.to(device)\n",
    "\n",
    "model = SymptomToDeptGNN(hidden_dim=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"开始训练 ...\")\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    logits = model(data.x_dict, data.edge_index_dict)     # [num_symptom, 54]\n",
    "    loss   = loss_fn(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = logits[test_idx].argmax(dim=1)\n",
    "            acc  = (pred == labels[test_idx]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"gnn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SymptomMLP(nn.Module):\n",
    "    def __init__(self, in_dim=768, hidden=128, out_dim=54):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 加载数据\n",
    "symptom_feat = torch.load(\"symptom_feat.pt\")  # shape: [N, 768]\n",
    "symptom_label = torch.load(\"symptom_label.pt\")  # shape: [N]\n",
    "mask = torch.load(\"mask.pt\")  # shape: [N]\n",
    "\n",
    "x = symptom_feat[mask]\n",
    "y = symptom_label[mask]\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(x)), test_size=0.2, random_state=42)\n",
    "\n",
    "# 模型、优化器\n",
    "mlp_model = SymptomMLP().to(device)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "print(\"🚀 开始训练 MLP ...\")\n",
    "for epoch in range(1, 101):\n",
    "    mlp_model.train()\n",
    "    logits = mlp_model(x[train_idx])\n",
    "    loss = loss_fn(logits, y[train_idx])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        mlp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = mlp_model(x[test_idx]).argmax(dim=1)\n",
    "            acc = (pred == y[test_idx]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_model.state_dict(), \"mlp_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\2690167770.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  symptom_feat = torch.load(\"symptom_feat.pt\")\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\2690167770.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mlp_model.load_state_dict(torch.load(\"mlp_model.pt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"symptom_name2idx.pkl\", \"rb\") as f:\n",
    "    symptom_name2idx = pickle.load(f)\n",
    "with open(\"department_idx2name.pkl\", \"rb\") as f:\n",
    "    dept_idx2name = pickle.load(f)\n",
    "\n",
    "symptom_feat = torch.load(\"symptom_feat.pt\")\n",
    "mlp_model = SymptomMLP().to(device)\n",
    "mlp_model.load_state_dict(torch.load(\"mlp_model.pt\", map_location=device))\n",
    "mlp_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_symptom_vector(name):\n",
    "    \"\"\"从缓存中查找症状向量\"\"\"\n",
    "    if name in symptom_name2idx:\n",
    "        idx = symptom_name2idx[name]\n",
    "        return symptom_feat[idx].unsqueeze(0)\n",
    "    else:\n",
    "        print(f\"❗症状 `{name}` 未命中缓存，忽略\")\n",
    "        return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_mlp(symptom_names, k=5):\n",
    "    vecs = [get_symptom_vector(name) for name in symptom_names]\n",
    "    vecs = [v for v in vecs if v is not None]\n",
    "\n",
    "    if not vecs:\n",
    "        print(\"❗无有效症状输入，无法预测\")\n",
    "        return []\n",
    "\n",
    "    input_tensor = torch.mean(torch.cat(vecs, dim=0), dim=0, keepdim=True).to(device)\n",
    "    logits = mlp_model(input_tensor)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    topk = torch.topk(probs, k=k)\n",
    "\n",
    "    return [(dept_idx2name[int(idx)], float(score)) for idx, score in zip(topk.indices[0], topk.values[0])]\n",
    "\n",
    "input_symptoms = [\"发烧\", \"咳嗽\", \"乏力\", \"头痛\", \"恶心\"]\n",
    "results = predict_with_mlp(input_symptoms)\n",
    "\n",
    "print(f\"综合推荐科室（根据症状：{', '.join(input_symptoms)}）：\")\n",
    "for i, (dept, score) in enumerate(results):\n",
    "    print(f\"  Top-{i+1}: {dept} ({score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
