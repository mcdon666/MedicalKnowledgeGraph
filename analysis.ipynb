{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "\n",
    "# è¿æ¥æ•°æ®åº“\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\n",
    "\n",
    "# æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹\n",
    "query = \"\"\"\n",
    "MATCH (a)-[r]->(b)\n",
    "RETURN id(a) as source, id(b) as target, type(r) as relation, properties(r) as edge_attr\n",
    "\"\"\"\n",
    "edges = graph.run(query).data()\n",
    "\n",
    "# æ„å»º NetworkX å›¾\n",
    "G = nx.DiGraph()\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[\"source\"], edge[\"target\"], **(edge[\"edge_attr\"] or {}))\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹å±æ€§\n",
    "node_query = \"MATCH (n) RETURN id(n) as id, labels(n) as labels, properties(n) as attr\"\n",
    "nodes = graph.run(node_query).data()\n",
    "\n",
    "for node in nodes:\n",
    "    G.add_node(node[\"id\"], **node[\"attr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç»“æ„ç»Ÿè®¡ä¿¡æ¯\n",
      "ğŸ‘‰ èŠ‚ç‚¹æ€»æ•°ï¼š44112\n",
      "ğŸ‘‰ è¾¹æ€»æ•°ï¼š276586\n",
      "ğŸ‘‰ å¹³å‡å…¥åº¦ï¼š6.27\n",
      "ğŸ‘‰ å¹³å‡å‡ºåº¦ï¼š6.27\n",
      "ğŸ‘‰ å„ç±»å‹èŠ‚ç‚¹æ•°é‡ï¼š\n",
      "   - Disease: 8808 ä¸ª\n",
      "   - Drug: 3828 ä¸ª\n",
      "   - Food: 4870 ä¸ª\n",
      "   - Check: 3353 ä¸ª\n",
      "   - Department: 54 ä¸ª\n",
      "   - Producer: 17201 ä¸ª\n",
      "   - Symptom: 5998 ä¸ª\n",
      "ğŸ‘‰ å„ç±»å‹å…³ç³»æ•°é‡ï¼š\n",
      "   - recommand_eat: 40236 æ¡\n",
      "   - no_eat: 22247 æ¡\n",
      "   - do_eat: 22238 æ¡\n",
      "   - belongs_to: 8844 æ¡\n",
      "   - common_drug: 14649 æ¡\n",
      "   - drugs_of: 17315 æ¡\n",
      "   - recommand_drug: 59467 æ¡\n",
      "   - need_check: 39423 æ¡\n",
      "   - has_symptom: 54717 æ¡\n",
      "   - acompany_with: 12029 æ¡\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# å›¾ç»“æ„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯è¾“å‡º\n",
    "# -----------------------------\n",
    "\n",
    "print(\"âœ… å›¾ç»“æ„ç»Ÿè®¡ä¿¡æ¯\")\n",
    "print(f\"ğŸ‘‰ èŠ‚ç‚¹æ€»æ•°ï¼š{G.number_of_nodes()}\")\n",
    "print(f\"ğŸ‘‰ è¾¹æ€»æ•°ï¼š{G.number_of_edges()}\")\n",
    "\n",
    "# èŠ‚ç‚¹å…¥åº¦ã€å‡ºåº¦\n",
    "in_degrees = dict(G.in_degree())\n",
    "out_degrees = dict(G.out_degree())\n",
    "\n",
    "avg_in_degree = sum(in_degrees.values()) / len(in_degrees)\n",
    "avg_out_degree = sum(out_degrees.values()) / len(out_degrees)\n",
    "\n",
    "print(f\"ğŸ‘‰ å¹³å‡å…¥åº¦ï¼š{avg_in_degree:.2f}\")\n",
    "print(f\"ğŸ‘‰ å¹³å‡å‡ºåº¦ï¼š{avg_out_degree:.2f}\")\n",
    "\n",
    "# èŠ‚ç‚¹æ ‡ç­¾ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰\n",
    "label_count = {}\n",
    "for node in nodes:\n",
    "    for label in node[\"labels\"]:\n",
    "        label_count[label] = label_count.get(label, 0) + 1\n",
    "print(\"ğŸ‘‰ å„ç±»å‹èŠ‚ç‚¹æ•°é‡ï¼š\")\n",
    "for label, count in label_count.items():\n",
    "    print(f\"   - {label}: {count} ä¸ª\")\n",
    "\n",
    "# å…³ç³»ç±»å‹ç»Ÿè®¡\n",
    "relation_count = {}\n",
    "for edge in edges:\n",
    "    rel = edge['relation']\n",
    "    relation_count[rel] = relation_count.get(rel, 0) + 1\n",
    "print(\"ğŸ‘‰ å„ç±»å‹å…³ç³»æ•°é‡ï¼š\")\n",
    "for rel, count in relation_count.items():\n",
    "    print(f\"   - {rel}: {count} æ¡\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‰ éƒ¨é—¨åˆ—è¡¨ï¼ˆå…± 54 ä¸ªï¼‰ï¼šä¸å­•ä¸è‚²ã€ä¸­åŒ»ç§‘ã€ä¸­åŒ»ç»¼åˆã€äº”å®˜ç§‘ã€äº§ç§‘ã€ä¼ æŸ“ç§‘ã€å„¿ç§‘ã€å„¿ç§‘ç»¼åˆã€å…¶ä»–ç§‘å®¤ã€å…¶ä»–ç»¼åˆã€å†…åˆ†æ³Œç§‘ã€å†…ç§‘ã€å‡è‚¥ã€å£è…”ç§‘ã€å‘¼å¸å†…ç§‘ã€å¤–ç§‘ã€å¦‡äº§ç§‘ã€å¦‡ç§‘ã€å°å„¿å†…ç§‘ã€å°å„¿å¤–ç§‘ã€åº·å¤ç§‘ã€å¿ƒå†…ç§‘ã€å¿ƒç†ç§‘ã€å¿ƒèƒ¸å¤–ç§‘ã€æ€¥è¯Šç§‘ã€æ€§ç—…ç§‘ã€æ„ŸæŸ“ç§‘ã€æ•´å½¢ç¾å®¹ç§‘ã€æ™®å¤–ç§‘ã€æ³Œå°¿å†…ç§‘ã€æ³Œå°¿å¤–ç§‘ã€æ¶ˆåŒ–å†…ç§‘ã€çƒ§ä¼¤ç§‘ã€ç”Ÿæ®–å¥åº·ã€ç”·ç§‘ã€çš®è‚¤æ€§ç—…ç§‘ã€çš®è‚¤ç§‘ã€çœ¼ç§‘ã€ç¥ç»å†…ç§‘ã€ç¥ç»å¤–ç§‘ã€ç²¾ç¥ç§‘ã€è€³é¼»å–‰ç§‘ã€è‚›è‚ ç§‘ã€è‚ç—…ã€è‚èƒ†å¤–ç§‘ã€è‚¾å†…ç§‘ã€è‚¿ç˜¤å†…ç§‘ã€è‚¿ç˜¤å¤–ç§‘ã€è‚¿ç˜¤ç§‘ã€è¥å…»ç§‘ã€è¡€æ¶²ç§‘ã€é—ä¼ ç—…ç§‘ã€é£æ¹¿å…ç–«ç§‘ã€éª¨å¤–ç§‘\n"
     ]
    }
   ],
   "source": [
    "# æ”¶é›†æ‰€æœ‰ Department ç±»å‹çš„éƒ¨é—¨å\n",
    "department_names = []\n",
    "for node in nodes:\n",
    "    if \"Department\" in node[\"labels\"]:\n",
    "        dept_name = node[\"attr\"].get(\"name\") or node[\"attr\"].get(\"department_name\")\n",
    "        if dept_name:\n",
    "            department_names.append(dept_name)\n",
    "\n",
    "# æŒ‰å­—å…¸åºæ’åºéƒ¨é—¨å\n",
    "department_names.sort()\n",
    "\n",
    "# æ‹¼æ¥æˆä¸€ä¸ªç”¨é¡¿å·åˆ†éš”çš„å­—ç¬¦ä¸²\n",
    "departments_str = \"ã€\".join(department_names)\n",
    "\n",
    "print(f\"ğŸ‘‰ éƒ¨é—¨åˆ—è¡¨ï¼ˆå…± {len(department_names)} ä¸ªï¼‰ï¼š{departments_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®ç»Ÿè®¡\n",
      "ç—‡çŠ¶ (Symptom)èŠ‚ç‚¹æ•°ï¼š5998\n",
      "ç–¾ç—… (Disease)èŠ‚ç‚¹æ•°ï¼š8765\n",
      "ç§‘å®¤ (Department)èŠ‚ç‚¹æ•°ï¼š48\n",
      "ç—‡çŠ¶-ç–¾ç—…è¾¹æ•°ï¼š54717\n",
      "ç–¾ç—…-ç§‘å®¤è¾¹æ•°ï¼š54717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# å…¨æµç¨‹ï¼šç—‡çŠ¶ â†’ ç–¾ç—… â†’ ç§‘å®¤é¢„æµ‹æ¨¡å‹æ„å»º\n",
    "# ä½¿ç”¨ PyTorch Geometric + Neo4j + BERT ç‰¹å¾\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from py2neo import Graph\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# ========== 1. è¿æ¥ Neo4j å¹¶æå–æ•°æ® ==========\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"12345678\"))\n",
    "\n",
    "# æå–ç—‡çŠ¶-ç–¾ç—…-ç§‘å®¤ä¸‰è·³è·¯å¾„\n",
    "sym_dis_dept_query = \"\"\"\n",
    "MATCH (s:Symptom)<-[:has_symptom]-(d:Disease)-[:belongs_to]->(dept:Department)\n",
    "RETURN id(s) as sid, s.name as sname, id(d) as did, d.name as dname, id(dept) as dept_id, dept.name as dept_name\n",
    "\"\"\"\n",
    "results = graph.run(sym_dis_dept_query).data()\n",
    "\n",
    "# æ„å»ºå®ä½“å­—å…¸ä¸æ˜ å°„\n",
    "symptoms, diseases, departments = {}, {}, {}\n",
    "edges_sym_dis, edges_dis_dept = [], []\n",
    "\n",
    "for row in results:\n",
    "    sid, sname = row[\"sid\"], row[\"sname\"]\n",
    "    did, dname = row[\"did\"], row[\"dname\"]\n",
    "    dept_id, dept_name = row[\"dept_id\"], row[\"dept_name\"]\n",
    "    symptoms[sid] = sname\n",
    "    diseases[did] = dname\n",
    "    departments[dept_id] = dept_name\n",
    "    edges_sym_dis.append((sid, did))\n",
    "    edges_dis_dept.append((did, dept_id))\n",
    "\n",
    "symptom_id_map = {nid: i for i, nid in enumerate(symptoms)}\n",
    "disease_id_map = {nid: i for i, nid in enumerate(diseases)}\n",
    "department_id_map = {nid: i for i, nid in enumerate(departments)}\n",
    "\n",
    "# ========= ç»Ÿè®¡ä¿¡æ¯ =========\n",
    "print(\"æ•°æ®ç»Ÿè®¡\")\n",
    "print(f\"ç—‡çŠ¶ (Symptom)èŠ‚ç‚¹æ•°ï¼š{len(symptoms)}\")\n",
    "print(f\"ç–¾ç—… (Disease)èŠ‚ç‚¹æ•°ï¼š{len(diseases)}\")\n",
    "print(f\"ç§‘å®¤ (Department)èŠ‚ç‚¹æ•°ï¼š{len(departments)}\")\n",
    "print(f\"ç—‡çŠ¶-ç–¾ç—…è¾¹æ•°ï¼š{len(edges_sym_dis)}\")\n",
    "print(f\"ç–¾ç—…-ç§‘å®¤è¾¹æ•°ï¼š{len(edges_dis_dept)}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»º department ç´¢å¼• â†’ åç§° æ˜ å°„\n",
    "department_idx2name = {\n",
    "    idx: departments[nid]  # nid æ˜¯ Neo4j ä¸­çš„èŠ‚ç‚¹ IDï¼Œdepartments[nid] æ˜¯åç§°\n",
    "    for nid, idx in department_id_map.items()\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open(\"department_idx2name.pkl\", \"wb\") as f:\n",
    "    pickle.dump(department_idx2name, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 2. æ„å»º HeteroData å¼‚æ„å›¾ ==========\n",
    "data = HeteroData()\n",
    "data['symptom'].num_nodes = len(symptoms)\n",
    "data['disease'].num_nodes = len(diseases)\n",
    "data['department'].num_nodes = len(departments)\n",
    "\n",
    "# è¾¹ï¼šsymptom <-> disease\n",
    "edge_index_sd = torch.tensor([[symptom_id_map[s], disease_id_map[d]] for s, d in edges_sym_dis], dtype=torch.long).t()\n",
    "data['symptom', 'has_symptom', 'disease'].edge_index = edge_index_sd\n",
    "\n",
    "# è¾¹ï¼šdisease <-> department\n",
    "edge_index_dd = torch.tensor([[disease_id_map[d], department_id_map[dept]] for d, dept in edges_dis_dept], dtype=torch.long).t()\n",
    "data['disease', 'belongs_to', 'department'].edge_index = edge_index_dd\n",
    "\n",
    "# è‡ªåŠ¨æ·»åŠ åå‘è¾¹\n",
    "data = ToUndirected()(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, \"data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¼–ç  ç—‡çŠ¶ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç—‡çŠ¶:   0%|          | 0/5998 [00:00<?, ?it/s]g:\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "ç—‡çŠ¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5998/5998 [00:38<00:00, 156.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²ä¿å­˜åˆ° symptom_feat.pt å’Œ symptom_name2idx.pkl\n",
      "ç¼–ç  ç–¾ç—… ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç–¾ç—…: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8765/8765 [00:56<00:00, 156.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²ä¿å­˜åˆ° disease_feat.pt å’Œ disease_name2idx.pkl\n",
      "ç¼–ç  ç§‘å®¤ ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç§‘å®¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:00<00:00, 146.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²ä¿å­˜åˆ° department_feat.pt å’Œ department_name2idx.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# ========== 3. ç”Ÿæˆ BERT ç‰¹å¾ ==========\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "model.eval()  # å…³é—­ dropout ç­‰è®­ç»ƒæ€\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_bert_embedding(text: str) -> torch.Tensor:\n",
    "    \"\"\"å¯¹å•ä¸ªæ–‡æœ¬è¿”å› [CLS] å‘é‡ (1, 768)ã€‚\"\"\"\n",
    "    inputs  = tokenizer(text, return_tensors='pt', truncation=True,\n",
    "                        padding=True, max_length=10).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]    # shape: (1, 768)\n",
    "\n",
    "def build_or_load(name_dict, save_path, desc, name2idx_path):\n",
    "    if os.path.exists(save_path) and os.path.exists(name2idx_path):\n",
    "        print(f\"è½½å…¥ç¼“å­˜ç‰¹å¾ï¼š{save_path}\")\n",
    "        feats = torch.load(save_path)\n",
    "        with open(name2idx_path, \"rb\") as f:\n",
    "            name2idx = pickle.load(f)\n",
    "        return feats, name2idx\n",
    "\n",
    "    print(f\"ç¼–ç  {desc} ...\")\n",
    "    feats = []\n",
    "    name2idx = {}\n",
    "    for i, (nid, name) in enumerate(tqdm(name_dict.items(), total=len(name_dict), desc=desc)):\n",
    "        vec = get_bert_embedding(name).cpu()\n",
    "        feats.append(vec.squeeze(0))\n",
    "        name2idx[name] = i\n",
    "\n",
    "    feats = torch.stack(feats)\n",
    "    torch.save(feats, save_path)\n",
    "    with open(name2idx_path, \"wb\") as f:\n",
    "        pickle.dump(name2idx, f)\n",
    "    print(f\"å·²ä¿å­˜åˆ° {save_path} å’Œ {name2idx_path}\")\n",
    "    return feats, name2idx\n",
    "\n",
    "symptom_feat, symptom_name2idx = build_or_load(symptoms, \"symptom_feat.pt\", \"ç—‡çŠ¶\", \"symptom_name2idx.pkl\")\n",
    "disease_feat, disease_name2idx = build_or_load(diseases, \"disease_feat.pt\", \"ç–¾ç—…\", \"disease_name2idx.pkl\")\n",
    "department_feat, department_name2idx = build_or_load(departments, \"department_feat.pt\", \"ç§‘å®¤\", \"department_name2idx.pkl\")\n",
    "\n",
    "\n",
    "data['symptom'].x = symptom_feat\n",
    "data['disease'].x = disease_feat\n",
    "data['department'].x = department_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…±æ„å»ºæœ‰æ•ˆæ ‡ç­¾æ ·æœ¬æ•°ï¼š5998\n"
     ]
    }
   ],
   "source": [
    "# æ„é€  symptom â†’ department çš„æ ‡ç­¾åˆ—è¡¨\n",
    "symptom_label = torch.full((len(symptoms),), -1, dtype=torch.long)  # -1 è¡¨ç¤ºæ— æ ‡ç­¾ï¼ˆè·³è¿‡ï¼‰\n",
    "\n",
    "# åå‘æ„å»ºï¼šé€šè¿‡ç–¾ç—…è¿æ¥çš„ç§‘å®¤\n",
    "for (s_id, d_id) in edges_sym_dis:\n",
    "    dept_ids = [dept for (did, dept) in edges_dis_dept if did == d_id]\n",
    "    if len(dept_ids) == 0:\n",
    "        continue\n",
    "    s_idx = symptom_id_map[s_id]\n",
    "    # æ³¨æ„ï¼šå­˜åœ¨å¤šä¸ªç–¾ç—…è¿æ¥åˆ°ä¸åŒç§‘å®¤çš„æƒ…å†µï¼Œä»…å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿå¯ä»¥æ”¹ä¸ºå¤šæ ‡ç­¾ one-hotï¼‰\n",
    "    dept_idx = department_id_map[dept_ids[0]]\n",
    "    symptom_label[s_idx] = dept_idx\n",
    "\n",
    "# å»é™¤æ— æ ‡ç­¾çš„æ ·æœ¬\n",
    "mask = symptom_label != -1\n",
    "labels = symptom_label[mask]\n",
    "print(f\"å…±æ„å»ºæœ‰æ•ˆæ ‡ç­¾æ ·æœ¬æ•°ï¼š{labels.size(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(symptom_label, \"symptom_label.pt\")\n",
    "torch.save(mask, \"mask.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\3342686644.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  symptom_label = torch.load(\"symptom_label.pt\")\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\3342686644.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mask = torch.load(\"mask.pt\")\n"
     ]
    }
   ],
   "source": [
    "symptom_label = torch.load(\"symptom_label.pt\")\n",
    "mask = torch.load(\"mask.pt\")\n",
    "labels = symptom_label[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†æ ·æœ¬: 4798ï¼Œæµ‹è¯•é›†æ ·æœ¬: 1200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# æœ‰æ•ˆç—‡çŠ¶ç´¢å¼•ï¼ˆmask=True å¤„ï¼‰\n",
    "valid_sym_idx = torch.where(mask)[0]        # 5998 å…¨éƒ¨æœ‰æ•ˆ\n",
    "train_idx, test_idx = train_test_split(valid_sym_idx.numpy(),\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=42,\n",
    "                                       shuffle=True)\n",
    "\n",
    "train_idx = torch.tensor(train_idx, dtype=torch.long)\n",
    "test_idx  = torch.tensor(test_idx,  dtype=torch.long)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†æ ·æœ¬: {len(train_idx)}ï¼Œæµ‹è¯•é›†æ ·æœ¬: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymptomToDeptGNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, out_dim=len(department_id_map)):\n",
    "        super().__init__()\n",
    "        self.conv = HeteroConv({\n",
    "            ('symptom', 'has_symptom', 'disease'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "            ('disease', 'rev_has_symptom', 'symptom'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "            ('disease', 'belongs_to', 'department'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "            ('department','rev_belongs_to','disease'): GATConv((-1, -1), hidden_dim, add_self_loops=False),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv(x_dict, edge_index_dict)    # å¼‚æ„æ¶ˆæ¯ä¼ æ’­\n",
    "        x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "        return self.lin(x_dict['symptom'])             # ä»…è¾“å‡ºç—‡çŠ¶èŠ‚ç‚¹ logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹è®­ç»ƒ ...\n",
      "Epoch 001 | Loss: 3.9007 | Test Acc: 0.0283\n",
      "Epoch 010 | Loss: 2.4937 | Test Acc: 0.3225\n",
      "Epoch 020 | Loss: 1.7720 | Test Acc: 0.4783\n",
      "Epoch 030 | Loss: 1.3902 | Test Acc: 0.5650\n",
      "Epoch 040 | Loss: 1.1775 | Test Acc: 0.5933\n",
      "Epoch 050 | Loss: 1.0255 | Test Acc: 0.6183\n",
      "Epoch 060 | Loss: 0.9062 | Test Acc: 0.6167\n",
      "Epoch 070 | Loss: 0.8177 | Test Acc: 0.6283\n",
      "Epoch 080 | Loss: 0.7301 | Test Acc: 0.6333\n",
      "Epoch 090 | Loss: 0.6662 | Test Acc: 0.6317\n",
      "Epoch 100 | Loss: 0.6098 | Test Acc: 0.6433\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "labels = labels.to(device)            # size = 5998\n",
    "train_idx = train_idx.to(device)\n",
    "test_idx  = test_idx.to(device)\n",
    "\n",
    "model = SymptomToDeptGNN(hidden_dim=128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒ ...\")\n",
    "for epoch in range(1, 101):\n",
    "    model.train()\n",
    "    logits = model(data.x_dict, data.edge_index_dict)     # [num_symptom, 54]\n",
    "    loss   = loss_fn(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = logits[test_idx].argmax(dim=1)\n",
    "            acc  = (pred == labels[test_idx]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "torch.save(model.state_dict(), \"gnn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SymptomMLP(nn.Module):\n",
    "    def __init__(self, in_dim=768, hidden=128, out_dim=54):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "symptom_feat = torch.load(\"symptom_feat.pt\")  # shape: [N, 768]\n",
    "symptom_label = torch.load(\"symptom_label.pt\")  # shape: [N]\n",
    "mask = torch.load(\"mask.pt\")  # shape: [N]\n",
    "\n",
    "x = symptom_feat[mask]\n",
    "y = symptom_label[mask]\n",
    "\n",
    "train_idx, test_idx = train_test_split(range(len(x)), test_size=0.2, random_state=42)\n",
    "\n",
    "# æ¨¡å‹ã€ä¼˜åŒ–å™¨\n",
    "mlp_model = SymptomMLP().to(device)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "x, y = x.to(device), y.to(device)\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒ MLP ...\")\n",
    "for epoch in range(1, 101):\n",
    "    mlp_model.train()\n",
    "    logits = mlp_model(x[train_idx])\n",
    "    loss = loss_fn(logits, y[train_idx])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        mlp_model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = mlp_model(x[test_idx]).argmax(dim=1)\n",
    "            acc = (pred == y[test_idx]).float().mean().item()\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | Test Acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mlp_model.state_dict(), \"mlp_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\2690167770.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  symptom_feat = torch.load(\"symptom_feat.pt\")\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22496\\2690167770.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mlp_model.load_state_dict(torch.load(\"mlp_model.pt\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"symptom_name2idx.pkl\", \"rb\") as f:\n",
    "    symptom_name2idx = pickle.load(f)\n",
    "with open(\"department_idx2name.pkl\", \"rb\") as f:\n",
    "    dept_idx2name = pickle.load(f)\n",
    "\n",
    "symptom_feat = torch.load(\"symptom_feat.pt\")\n",
    "mlp_model = SymptomMLP().to(device)\n",
    "mlp_model.load_state_dict(torch.load(\"mlp_model.pt\", map_location=device))\n",
    "mlp_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_symptom_vector(name):\n",
    "    \"\"\"ä»ç¼“å­˜ä¸­æŸ¥æ‰¾ç—‡çŠ¶å‘é‡\"\"\"\n",
    "    if name in symptom_name2idx:\n",
    "        idx = symptom_name2idx[name]\n",
    "        return symptom_feat[idx].unsqueeze(0)\n",
    "    else:\n",
    "        print(f\"â—ç—‡çŠ¶ `{name}` æœªå‘½ä¸­ç¼“å­˜ï¼Œå¿½ç•¥\")\n",
    "        return None\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_mlp(symptom_names, k=5):\n",
    "    vecs = [get_symptom_vector(name) for name in symptom_names]\n",
    "    vecs = [v for v in vecs if v is not None]\n",
    "\n",
    "    if not vecs:\n",
    "        print(\"â—æ— æœ‰æ•ˆç—‡çŠ¶è¾“å…¥ï¼Œæ— æ³•é¢„æµ‹\")\n",
    "        return []\n",
    "\n",
    "    input_tensor = torch.mean(torch.cat(vecs, dim=0), dim=0, keepdim=True).to(device)\n",
    "    logits = mlp_model(input_tensor)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    topk = torch.topk(probs, k=k)\n",
    "\n",
    "    return [(dept_idx2name[int(idx)], float(score)) for idx, score in zip(topk.indices[0], topk.values[0])]\n",
    "\n",
    "input_symptoms = [\"å‘çƒ§\", \"å’³å—½\", \"ä¹åŠ›\", \"å¤´ç—›\", \"æ¶å¿ƒ\"]\n",
    "results = predict_with_mlp(input_symptoms)\n",
    "\n",
    "print(f\"ç»¼åˆæ¨èç§‘å®¤ï¼ˆæ ¹æ®ç—‡çŠ¶ï¼š{', '.join(input_symptoms)}ï¼‰ï¼š\")\n",
    "for i, (dept, score) in enumerate(results):\n",
    "    print(f\"  Top-{i+1}: {dept} ({score:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
